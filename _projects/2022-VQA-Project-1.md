---
title: "VQA: Asking Questions About Image Content"
collection: projects
permalink: /projects/2022-VQA-Project-1
excerpt: "A multimodal Visual Question Answering project combining CNN/ViT image encoders with LSTM/GRU/BERT language models."
date: 2022-08-23
venue: "North South University"
---

This project explores **Visual Question Answering (VQA)**, where a model answers natural-language questions based on image content.

I implemented and evaluated two multimodal pipelines:

- **Pipeline 1:** CNN-based image encoding + LSTM/GRU/CNN-based text modeling
- **Pipeline 2:** Vision Transformer (ViT) image encoding + BERT-based text modeling

The objective was to compare architecture behavior on open-ended and constrained-answer VQA tasks and study how representation quality impacts answer accuracy.

![Project Image](/files/projects/project1_capstone_project_poster.png)

[Project Demo](https://huggingface.co/Zayn/VQA_Asking_Questions_About_Image_Content)

[Download Capstone Poster](http://hashcatnissan.github.io/files/projects/project_1_Capstone_Poster.pdf)

Supervisor: Prof. Mohammad Ashrafuzzman Khan  
Authors: Nishan, Ashfaq Uddin Ahmed, Tashfia Tabassum

